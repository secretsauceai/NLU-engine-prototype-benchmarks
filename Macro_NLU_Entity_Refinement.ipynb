{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nlu_engine import NLUEngine\n",
    "from nlu_engine import MacroDataRefinement\n",
    "from nlu_engine import MacroEntityRefinement\n",
    "\n",
    "from nlu_engine import DataUtils\n",
    "from nlu_engine import RenderJSON\n",
    "\n",
    "from nlu_engine import Analytics\n",
    "\n",
    "from nlu_engine import EntityExtractor, crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, you’ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "NOTE: We will use Logistic Regression with TFIDF features to train our intent models and CRFs for entity exraction. Why? Well, they are very fast and both methods aren't state-of-the-art. This is good, because it is easier to find problems we will need to refine in the dataset than if we were to use a proper NLU engine like Snips or something SOTA like BERT. It is very important to note that some of the the problems we will pick up on, might not be an actual issue, but might be due to the limitations of the models. Refining the real problems and ignoring the limitations of the models is a good way to improve the models. Then when the dataset is ready, we can use some more advanced NLU engine and get the best performance possible.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded nlu_data_refined_df.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlu_data_df = pd.read_csv(\n",
    "        'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)\n",
    "    print('Successfully loaded nlu_data_refined_df.csv')\n",
    "except:\n",
    "    data = 'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    "    nlu_data_df = DataUtils.load_data(\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `nlu_data_df['answer_normalised']` is taken from `nlu_data_df['answer_annotation']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = DataUtils.convert_annotated_utterances_to_normalised_utterances(\n",
    "    nlu_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>answerid</th>\n",
       "      <th>notes</th>\n",
       "      <th>question</th>\n",
       "      <th>suggested_entities</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>intent_refined</th>\n",
       "      <th>entity_refined</th>\n",
       "      <th>remove</th>\n",
       "      <th>status</th>\n",
       "      <th>answer_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>wake me up at 5am this week</td>\n",
       "      <td>wake me up at five am this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : five am] [date : this week]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>wake me up at 9am on Friday</td>\n",
       "      <td>wake me up at nine am on friday</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : nine am] on [date : friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set an alarm for [time : two hours from now]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>player_setting</td>\n",
       "      <td>Olly quiet!</td>\n",
       "      <td>quiet</td>\n",
       "      <td>audio</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quiet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>player_setting</td>\n",
       "      <td>Olly pause for ten seconds</td>\n",
       "      <td>pause for ten seconds</td>\n",
       "      <td>audio</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pause for [time : ten seconds]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>781.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25712</th>\n",
       "      <td>NaN</td>\n",
       "      <td>782.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25713</th>\n",
       "      <td>NaN</td>\n",
       "      <td>783.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25714</th>\n",
       "      <td>NaN</td>\n",
       "      <td>784.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25715</th>\n",
       "      <td>NaN</td>\n",
       "      <td>785.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25074 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  answerid notes  \\\n",
       "0         1.0       1.0   NaN   \n",
       "1         1.0       2.0   NaN   \n",
       "2         1.0       3.0   NaN   \n",
       "3         1.0      31.0   NaN   \n",
       "5         1.0      33.0   NaN   \n",
       "...       ...       ...   ...   \n",
       "25711     NaN     781.0   NaN   \n",
       "25712     NaN     782.0   NaN   \n",
       "25713     NaN     783.0   NaN   \n",
       "25714     NaN     784.0   NaN   \n",
       "25715     NaN     785.0   NaN   \n",
       "\n",
       "                                                question suggested_entities  \\\n",
       "0      Write what you would tell your PDA in the foll...         date, time   \n",
       "1      Write what you would tell your PDA in the foll...         date, time   \n",
       "2      Write what you would tell your PDA in the foll...         date, time   \n",
       "3      Write what you would tell your PDA in the foll...     player_setting   \n",
       "5      Write what you would tell your PDA in the foll...     player_setting   \n",
       "...                                                  ...                ...   \n",
       "25711                                                NaN                NaN   \n",
       "25712                                                NaN                NaN   \n",
       "25713                                                NaN                NaN   \n",
       "25714                                                NaN                NaN   \n",
       "25715                                                NaN                NaN   \n",
       "\n",
       "                                      answer  \\\n",
       "0                wake me up at 5am this week   \n",
       "1                wake me up at 9am on Friday   \n",
       "2        set an alarm for two hours from now   \n",
       "3                                Olly quiet!   \n",
       "5                 Olly pause for ten seconds   \n",
       "...                                      ...   \n",
       "25711  that's cool, musch appreciated, olly.   \n",
       "25712             you are hero, appreciated.   \n",
       "25713                   thanks, that's nice.   \n",
       "25714        that's cool, thank you so much.   \n",
       "25715         appreciate your answers, olly.   \n",
       "\n",
       "                           answer_normalised scenario       intent  \\\n",
       "0            wake me up at five am this week    alarm    alarm_set   \n",
       "1            wake me up at nine am on friday    alarm    alarm_set   \n",
       "2        set an alarm for two hours from now    alarm    alarm_set   \n",
       "3                                      quiet    audio  volume_mute   \n",
       "5                      pause for ten seconds    audio  volume_mute   \n",
       "...                                      ...      ...          ...   \n",
       "25711  that's cool, musch appreciated, olly.  general       praise   \n",
       "25712             you are hero, appreciated.  general       praise   \n",
       "25713                   thanks, that's nice.  general       praise   \n",
       "25714        that's cool, thank you so much.  general       praise   \n",
       "25715         appreciate your answers, olly.  general       praise   \n",
       "\n",
       "      predicted_label intent_refined  entity_refined remove status  \\\n",
       "0           alarm_set            NaN             NaN    NaN    NaN   \n",
       "1           alarm_set            NaN             NaN    NaN    NaN   \n",
       "2           alarm_set            NaN             NaN    NaN    NaN   \n",
       "3         volume_mute            NaN             NaN    NaN    NaN   \n",
       "5         volume_mute           True             NaN   True    NaN   \n",
       "...               ...            ...             ...    ...    ...   \n",
       "25711          praise            NaN             NaN    NaN    NaN   \n",
       "25712          praise            NaN             NaN    NaN    NaN   \n",
       "25713          praise            NaN             NaN    NaN    NaN   \n",
       "25714          praise            NaN             NaN    NaN    NaN   \n",
       "25715          praise            NaN             NaN    NaN    NaN   \n",
       "\n",
       "                                       answer_annotation  \n",
       "0      wake me up at [time : five am] [date : this week]  \n",
       "1      wake me up at [time : nine am] on [date : friday]  \n",
       "2           set an alarm for [time : two hours from now]  \n",
       "3                                                  quiet  \n",
       "5                         pause for [time : ten seconds]  \n",
       "...                                                  ...  \n",
       "25711              that's cool, musch appreciated, olly.  \n",
       "25712                         you are hero, appreciated.  \n",
       "25713                               thanks, that's nice.  \n",
       "25714                    that's cool, thank you so much.  \n",
       "25715                     appreciate your answers, olly.  \n",
       "\n",
       "[25074 rows x 15 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: does this NEED to be here, LOL!\n",
    "from matplotlib.pyplot import axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove the unwanted entries for the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_df[nlu_data_df['remove'] != True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity extraction reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement brown clustering to improve entity extraction (see entity_extractor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=removed_nlu_data_refined_df, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the report plotted for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(entity_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to extract out the entity types from the dataset. We need this to calculate the precentage of entities, to filter out any domains that have too few entities for benchmarking by domain instead of all domains together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "removed_nlu_data_refined_df['entities'] = removed_nlu_data_refined_df['answer_annotation'].apply(\n",
    "    EntityExtractor.extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "removed_nlu_data_refined_df['entity_types'] = removed_nlu_data_refined_df['entities'].apply(\n",
    "    EntityExtractor.extract_entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add this as a method to the entity extractor class (should I still do this? YES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "removed_nlu_data_refined_df['entity_types'] = removed_nlu_data_refined_df['entity_types'].apply(lambda y: np.nan if len(y)==0 else y)\n",
    "removed_nlu_data_refined_df['entities'] = removed_nlu_data_refined_df['entities'].apply(lambda y: np.nan if len(y)==0 else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = NLUEngine.get_entity_reports_for_domains(removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = domain_entity_reports_df[domain_entity_reports_df['entity-type'].str.contains(\n",
    "    'weighted avg')].sort_values(by='f1-score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: rename plot_report to plot_intent_report and this code below should be a method called plot_entity_report\n",
    "label = domain_entity_reports_df.columns[5]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "graph_report_df = domain_entity_reports_df.sort_values(\n",
    "            by='f1-score', ascending=True)\n",
    "y_axis = np.arange(len(graph_report_df[label]))\n",
    "\n",
    "ax.barh(y_axis, graph_report_df['f1-score'],\n",
    "        align='center', color='b')\n",
    "\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax.set_title(f'f1-scores by {label}', fontsize=24)\n",
    "ax.set_xlabel(\"f1-score\", fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.set_ylabel(label, fontsize=18)\n",
    "ax.set_yticks(y_axis, graph_report_df[label], fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity classification \n",
    "\n",
    "We can get the `predicted_tagging` and find all of the entities that are being incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have a saved model (for the default data set there is already one), you can train a model and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = NLUEngine.train_entity_classifier(removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/analytics/entity_tagger.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataUtils.pickle_model(classifier=crf_model, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open it from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/analytics/entity_tagger.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = DataUtils.import_pickled_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run it across our data set to get the `predicted_tagging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "removed_nlu_data_refined_df['predicted_tagging'] = removed_nlu_data_refined_df['answer_normalised'].apply(\n",
    "    lambda x: NLUEngine.create_entity_tagged_utterance(x, crf_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load a dataset of all of the incorrectly predicted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: I might move this out since it is only being done after the overlapping entities have been removed\n",
    "incorrect_predicted_entities_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation']\n",
    "                            != removed_nlu_data_refined_df['predicted_tagging']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted_entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific entity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new domain start here!\n",
    "\n",
    "Pick a domain to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(\n",
    "    removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataframe\n"
     ]
    }
   ],
   "source": [
    "domain_df = DataUtils.get_domain_df(\n",
    "    removed_nlu_data_refined_df, domain_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrectly predicted entities report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted_entities_report = MacroEntityRefinement.get_incorrect_predicted_entities_report(\n",
    "    domain_df, entity_report_df, domain_entity_reports_df)\n",
    "\n",
    "#TODO: Have user go through general refinement in batches of 100 or less per domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a better look at this report of incorrectly predicted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RenderJSON(incorrect_predicted_entities_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity overlap refinement\n",
    "We want to find the entities that have overlapping entity types. Entities (the words themselves that get tagged), should very rarely have more than one entity type in a domain. If they do, then we need to refine the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take the `domain_df['entities']` and get a list of dictionaries for:\n",
    "* `id`: the original index of the row\n",
    "* `entity_type`: the entity type\n",
    "* `entity_words`: the joined words with spaces that make up the entity\n",
    "\n",
    "and then drop it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_entity_types, incorrect_entity_types, overlapping_entity_words = MacroEntityRefinement.pick_correct_entity_type_for_overlapping_entities(\n",
    "    domain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the `correct_entity_types`. We will need those to know what to change the `entity_type` to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably get a list of the `incorrect_entity_types` to go along with our `correct_entity_types`, so we know what to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we for loop over the zip of `incorrect_entity_types` and `correct_entity_types` and replace the `entity_type` in `overlapping_domain_df` with the `correct_entity_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing: timeofday : every morning\n",
      "with: general_frequency : every morning\n",
      "replacing: alarm_type : soccer practice\n",
      "with: event_name : soccer practice\n"
     ]
    }
   ],
   "source": [
    "refined_overlapping_domain_df = MacroEntityRefinement.refine_overlapping_entity_types(\n",
    "    domain_df, correct_entity_types, incorrect_entity_types, overlapping_entity_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity refinement\n",
    "We will want to refine all of the entries that have the incorrect entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_incorrect_predicted_entities_df = refined_overlapping_domain_df[refined_overlapping_domain_df['answer_annotation']\n",
    "                                                              != refined_overlapping_domain_df['predicted_tagging']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bfe859247044adac7997a94832dd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sheet(cells=(Cell(column_end=0, column_start=0, numeric_format=None, row_end=43, row_start=0, squeeze_row=Fals…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_review_sheet = MacroDataRefinement.create_sheet(\n",
    "    domain_incorrect_predicted_entities_df)\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: start from here, refine then save to CSV the changes. Repeat until all domains have been refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_domain_entities_df = MacroDataRefinement.convert_sheet_to_dataframe(to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_domain_entities_df = reviewed_domain_entities_df.apply(\n",
    "    MacroDataRefinement.replace_annotated_utterance_with_predicted_tagging, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_domain_entities_df.to_csv(\n",
    "    f'data/refined/entities/{domain_selection}_refined_entities_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_domain_entities_df = MacroDataRefinement.mark_entries_as_refined(\n",
    "    refined_dataframe=refined_domain_entities_df, refined_type='entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df.update(refined_domain_entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df.to_csv(\n",
    "    f'data/removed_nlu_data_refined_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM HERE: loop back for each domain to refine the entities until done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: once all of the refinements of each domain have been completed the nlu_data_refined_df should be updated with the refined data from removed_nlu_data_refined_df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: refactor intent refinement to dump the csvs into a sub dir called intents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: It is probably a good idea to drop all of the ones that lack a good support.\n",
    "#NOTE: But it didn't work to fix the problem.\n",
    "remove_entities = [\n",
    "    'music_album',\n",
    "    'game_type',\n",
    "    \n",
    "]\n",
    "removed_nlu_data_refined__entities_cleaned_df = removed_nlu_data_refined_df[~removed_nlu_data_refined_df['answer_annotation'].str.contains('|'.join(remove_entities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(entity_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove/replace worst: add in state features like here: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
    "# Specifically, we want print_state_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu_engine import crf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model.state_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_state_features(Counter(crf.state_features_).most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: review the most common features, none of them are the word parts (chunks) or POS tags, are these even needed or helpful?\n",
    "# Can we remove them and speed up the process?\n",
    "# What other features could be used? Word2vec? Brown clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(nlu_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from the entity extraction report, the entity extraction is not working for the alarm_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: review all scoring 0, see if they can be completely dropped or what\n",
    "entity_to_refine = 'alarm_type'\n",
    "nlu_scenario_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation'].str.contains(\n",
    "    entity_to_refine)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_scenario_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entity(df, entity_to_remove):\n",
    "    \"\"\"\n",
    "        Remove all entries of an entity type from the dataframe.\n",
    "        :param df: pandas dataframe\n",
    "        :return: pandas dataframe\n",
    "        \"\"\"\n",
    "    updated_df = df.copy()\n",
    "    updated_df.loc[updated_df['answer_annotation'].str.contains(\n",
    "        entity_to_remove), 'remove'] = True\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = remove_entity(removed_nlu_data_refined_df, entity_to_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation'].str.contains(\n",
    "    entity_to_refine)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Convert to ipysheet and review\n",
    "TODO: add in description of the types of fixes we can do to the NLU data for entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: same as above for intents but with predicted entities: report on them, break them down into a dictionary of dataframes and refine them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with 'alarm' and the alarm_type:\n",
    "* We see that the alarm_type entities are really event_name(ie wake up, soccer practice) except for ID 5879, we will need to change them to event_name and remove ID 5879.\n",
    "* The last one(ID 6320) is a mistake. Someone got confused with the prompt and assumed alarm is a security system. This is out of scope for the alarm domain, as the alarms are ones set on a phone or other device. We will drop this utterance.\n",
    "Once you are done reviewing, you convert it back to a dataframe and check to make sure it looks okay.\n",
    "Let's change all alarm_type entities to event_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewed_scenario_df['answer_annotation'] = reviewed_scenario_df['answer_annotation'].str.replace(\n",
    "    'alarm_type', 'event_name')\n",
    "reviewed_scenario_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay dokey, now we can merge this with the original data set and see if it made a difference already(well of course it did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df.drop(\n",
    "    reviewed_scenario_df[reviewed_scenario_df['remove'] == True].index, inplace=True)\n",
    "\n",
    "reviewed_scenario_df = reviewed_scenario_df[~reviewed_scenario_df['remove'] == True]\n",
    "\n",
    "nlu_data_df.loc[nlu_data_df.index.intersection(\n",
    "    reviewed_scenario_df.index), 'answer_annotation'] = reviewed_scenario_df['answer_annotation']\n",
    "\n",
    "nlu_data_df[(nlu_data_df['scenario'].str.contains('alarm')) & (nlu_data_df['answer_annotation'].str.contains(\n",
    "    'event_name'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark changed data set\n",
    "TODO: repeat reports for the changed data set for domain and entities and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_reviewed_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=nlu_data_df)\n",
    "entity_reviewed_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure it is okay, you can save it as a csv file, make sure to name it correctly(i.e. `alarm_domain_first_review.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_scenario_df.to_csv('alarm_domain_first_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back up and check to make sure it looks okay. Make sure to give it the right name!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_domain_first_review_df = pd.read_csv(\n",
    "    'alarm_domain_first_review.csv', index_col=0)\n",
    "audio_domain_first_review_df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the evaluate_classifier in the NLU engine to check f1 score for intents and entities in the domain vs original NLU data of domain!\n",
    "# Value: benchmark!\n",
    "#TODO: implement a flow for getting the domains with the lowest f1 scores by intent/domain and entities and cleaning them by the order of the lowest f1 scores\n",
    "# TODO: concat all reviewed dfs and save to csv\n",
    "# TODO: add benchmark for whole NLU data set before and after cleaning! (by intents and domains!)\n",
    "# TODO: review the review marked entries\n",
    "# TODO: add new column for notes\n",
    "# TODO: change flow of review for only ones that should be reviewed, not all of the ones that have been changed (track changes by comparing against the original data set)\n",
    "# TODO: do the changed utterances have to be changed in other fields too or is it just enough for the tagged utterancve field?\n",
    "# TODO: add visualizations of domains, their intents, keywords in utterances, and entities to top\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
