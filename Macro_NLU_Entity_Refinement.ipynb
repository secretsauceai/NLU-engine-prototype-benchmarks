{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nlu_engine import NLUEngine\n",
    "from nlu_engine import MacroDataRefinement\n",
    "from nlu_engine import MacroEntityRefinement\n",
    "\n",
    "from nlu_engine import DataUtils\n",
    "from nlu_engine import RenderJSON\n",
    "\n",
    "from nlu_engine import Analytics\n",
    "\n",
    "from nlu_engine import EntityExtractor, crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, you’ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "NOTE: We will use Logistic Regression with TFIDF features to train our intent models and CRFs for entity exraction. Why? Well, they are very fast and both methods aren't state-of-the-art. This is good, because it is easier to find problems we will need to refine in the dataset than if we were to use a proper NLU engine like Snips or something SOTA like BERT. It is very important to note that some of the the problems we will pick up on, might not be an actual issue, but might be due to the limitations of the models. Refining the real problems and ignoring the limitations of the models is a good way to improve the models. Then when the dataset is ready, we can use some more advanced NLU engine and get the best performance possible.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded nlu_data_refined_updated.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    removed_nlu_data_refined_df = pd.read_csv(\n",
    "        'data/removed_nlu_data_refined_updated.csv', index_col=0)\n",
    "    print('Successfully loaded nlu_data_refined_updated.csv')\n",
    "except:\n",
    "    nlu_data_df = pd.read_csv(\n",
    "        'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)\n",
    "    print('Successfully loaded nlu_data_refined_df.csv')\n",
    "    \n",
    "    nlu_data_df = DataUtils.convert_annotated_utterances_to_normalised_utterances(\n",
    "        nlu_data_df)\n",
    "\n",
    "    removed_nlu_data_refined_df = nlu_data_df[nlu_data_df['remove'] != True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: does this NEED to be here, LOL!\n",
    "from matplotlib.pyplot import axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove the unwanted entries for the next few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity extraction reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement brown clustering to improve entity extraction (see entity_extractor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=removed_nlu_data_refined_df, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the report plotted for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(entity_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to extract out the entity types from the dataset. We need this to calculate the precentage of entities, to filter out any domains that have too few entities for benchmarking by domain instead of all domains together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df['entities'] = removed_nlu_data_refined_df['answer_annotation'].apply(\n",
    "    EntityExtractor.extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df['entity_types'] = removed_nlu_data_refined_df['entities'].apply(\n",
    "    EntityExtractor.extract_entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add this as a method to the entity extractor class (should I still do this? YES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "removed_nlu_data_refined_df['entity_types'] = removed_nlu_data_refined_df['entity_types'].apply(lambda y: np.nan if len(y)==0 else y)\n",
    "removed_nlu_data_refined_df['entities'] = removed_nlu_data_refined_df['entities'].apply(lambda y: np.nan if len(y)==0 else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = NLUEngine.get_entity_reports_for_domains(removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = domain_entity_reports_df[domain_entity_reports_df['entity-type'].str.contains(\n",
    "    'weighted avg')].sort_values(by='f1-score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: rename plot_report to plot_intent_report and this code below should be a method called plot_entity_report\n",
    "label = domain_entity_reports_df.columns[5]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "graph_report_df = domain_entity_reports_df.sort_values(\n",
    "            by='f1-score', ascending=True)\n",
    "y_axis = np.arange(len(graph_report_df[label]))\n",
    "\n",
    "ax.barh(y_axis, graph_report_df['f1-score'],\n",
    "        align='center', color='b')\n",
    "\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax.set_title(f'f1-scores by {label}', fontsize=24)\n",
    "ax.set_xlabel(\"f1-score\", fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.set_ylabel(label, fontsize=18)\n",
    "ax.set_yticks(y_axis, graph_report_df[label], fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity classification \n",
    "\n",
    "We can get the `predicted_tagging` and find all of the entities that are being incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have a saved model (for the default data set there is already one), you can train a model and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = NLUEngine.train_entity_classifier(removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/analytics/entity_tagger.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataUtils.pickle_model(classifier=crf_model, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open it from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/analytics/entity_tagger.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = DataUtils.import_pickled_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run it across our data set to get the `predicted_tagging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df['predicted_tagging'] = removed_nlu_data_refined_df['answer_normalised'].apply(\n",
    "    lambda x: NLUEngine.create_entity_tagged_utterance(x, crf_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load a dataset of all of the incorrectly predicted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: I might move this out since it is only being done after the overlapping entities have been removed\n",
    "incorrect_predicted_entities_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation']\n",
    "                            != removed_nlu_data_refined_df['predicted_tagging']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted_entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific entity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new domain start here!\n",
    "\n",
    "Pick a domain to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(\n",
    "    removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataframe\n"
     ]
    }
   ],
   "source": [
    "domain_df = DataUtils.get_domain_df(\n",
    "    removed_nlu_data_refined_df, domain_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrectly predicted entities report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted_entities_report = MacroEntityRefinement.get_incorrect_predicted_entities_report(\n",
    "    domain_df, entity_report_df, domain_entity_reports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a better look at this report of incorrectly predicted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RenderJSON(incorrect_predicted_entities_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: place_name -> house_place\n",
    "# Add in ability to change a whole entity type to another. e.g. place_name -> house_place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity overlap refinement\n",
    "We want to find the entities that have overlapping entity types. Entities (the words themselves that get tagged), should very rarely have more than one entity type in a domain. If they do, then we need to refine the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take the `domain_df['entities']` and get a list of dictionaries for:\n",
    "* `id`: the original index of the row\n",
    "* `entity_type`: the entity type\n",
    "* `entity_words`: the joined words with spaces that make up the entity\n",
    "\n",
    "and then drop it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_entity_types, incorrect_entity_types, overlapping_entity_words = MacroEntityRefinement.pick_correct_entity_type_for_overlapping_entities(\n",
    "    domain_df)\n",
    "\n",
    "    #TODO: add option to just press enter to skip picking a correct entity type (i.e. coffee type black but color black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the `correct_entity_types`. We will need those to know what to change the `entity_type` to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably get a list of the `incorrect_entity_types` to go along with our `correct_entity_types`, so we know what to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we for loop over the zip of `incorrect_entity_types` and `correct_entity_types` and replace the `entity_type` in `overlapping_domain_df` with the `correct_entity_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing: color_type : everyday\n",
      "with: general_frequency : everyday\n",
      "replacing: date : floor\n",
      "with: house_place : floor\n",
      "replacing: place_name : light\n",
      "with: device_type : light\n",
      "replacing: color_type : overhead\n",
      "with: house_place : overhead\n"
     ]
    }
   ],
   "source": [
    "refined_overlapping_domain_df = MacroEntityRefinement.refine_overlapping_entity_types(\n",
    "    domain_df, correct_entity_types, incorrect_entity_types, overlapping_entity_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity refinement\n",
    "We will want to refine all of the entries that have the incorrect entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_incorrect_predicted_entities_df = refined_overlapping_domain_df[refined_overlapping_domain_df['answer_annotation']\n",
    "                                                              != refined_overlapping_domain_df['predicted_tagging']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e8e18dbc534ada9f78ba1922a69cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sheet(cells=(Cell(column_end=0, column_start=0, numeric_format=None, row_end=194, row_start=0, squeeze_row=Fal…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_review_sheet = MacroDataRefinement.create_sheet(\n",
    "    domain_incorrect_predicted_entities_df)\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e8e18dbc534ada9f78ba1922a69cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sheet(cells=(Cell(column_end=0, column_start=0, numeric_format=None, row_end=194, row_start=0, squeeze_row=Fal…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_review_sheet = MacroDataRefinement.create_sheet(\n",
    "    domain_incorrect_predicted_entities_df)\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: start from here, refine then save to CSV the changes. Repeat until all domains have been refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_domain_entities_df = MacroDataRefinement.convert_sheet_to_dataframe(to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_domain_entities_df = reviewed_domain_entities_df.apply(\n",
    "    MacroDataRefinement.replace_annotated_utterance_with_predicted_tagging, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_domain_entities_df.to_csv(\n",
    "    f'data/refined/entities/{domain_selection}_refined_entities_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_domain_entities_df = MacroDataRefinement.mark_entries_as_refined(\n",
    "    refined_dataframe=refined_domain_entities_df, refined_type='entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df.update(refined_domain_entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df.to_csv(\n",
    "    f'data/removed_nlu_data_refined_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM HERE: loop back for each domain to refine the entities until done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: once all of the refinements of each domain have been completed the nlu_data_refined_df should be updated with the refined data from removed_nlu_data_refined_df!\n",
    "# Don't forget: The removed entries are no longer in the removed_nlu_data_refined_df so they need to be added back into the nlu_data_refined_df from the files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: refactor intent refinement to dump the csvs into a sub dir called intents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: It is probably a good idea to drop all of the ones that lack a good support.\n",
    "#NOTE: But it didn't work to fix the problem.\n",
    "remove_entities = [\n",
    "    'music_album',\n",
    "    'game_type',\n",
    "    \n",
    "]\n",
    "removed_nlu_data_refined__entities_cleaned_df = removed_nlu_data_refined_df[~removed_nlu_data_refined_df['answer_annotation'].str.contains('|'.join(remove_entities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(entity_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove/replace worst: add in state features like here: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
    "# Specifically, we want print_state_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu_engine import crf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model.state_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_state_features(Counter(crf.state_features_).most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: review the most common features, none of them are the word parts (chunks) or POS tags, are these even needed or helpful?\n",
    "# Can we remove them and speed up the process?\n",
    "# What other features could be used? Word2vec? Brown clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(nlu_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from the entity extraction report, the entity extraction is not working for the alarm_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: review all scoring 0, see if they can be completely dropped or what\n",
    "entity_to_refine = 'alarm_type'\n",
    "nlu_scenario_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation'].str.contains(\n",
    "    entity_to_refine)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_scenario_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entity(df, entity_to_remove):\n",
    "    \"\"\"\n",
    "        Remove all entries of an entity type from the dataframe.\n",
    "        :param df: pandas dataframe\n",
    "        :return: pandas dataframe\n",
    "        \"\"\"\n",
    "    updated_df = df.copy()\n",
    "    updated_df.loc[updated_df['answer_annotation'].str.contains(\n",
    "        entity_to_remove), 'remove'] = True\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = remove_entity(removed_nlu_data_refined_df, entity_to_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation'].str.contains(\n",
    "    entity_to_refine)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Convert to ipysheet and review\n",
    "TODO: add in description of the types of fixes we can do to the NLU data for entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: same as above for intents but with predicted entities: report on them, break them down into a dictionary of dataframes and refine them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with 'alarm' and the alarm_type:\n",
    "* We see that the alarm_type entities are really event_name(ie wake up, soccer practice) except for ID 5879, we will need to change them to event_name and remove ID 5879.\n",
    "* The last one(ID 6320) is a mistake. Someone got confused with the prompt and assumed alarm is a security system. This is out of scope for the alarm domain, as the alarms are ones set on a phone or other device. We will drop this utterance.\n",
    "Once you are done reviewing, you convert it back to a dataframe and check to make sure it looks okay.\n",
    "Let's change all alarm_type entities to event_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewed_scenario_df['answer_annotation'] = reviewed_scenario_df['answer_annotation'].str.replace(\n",
    "    'alarm_type', 'event_name')\n",
    "reviewed_scenario_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay dokey, now we can merge this with the original data set and see if it made a difference already(well of course it did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df.drop(\n",
    "    reviewed_scenario_df[reviewed_scenario_df['remove'] == True].index, inplace=True)\n",
    "\n",
    "reviewed_scenario_df = reviewed_scenario_df[~reviewed_scenario_df['remove'] == True]\n",
    "\n",
    "nlu_data_df.loc[nlu_data_df.index.intersection(\n",
    "    reviewed_scenario_df.index), 'answer_annotation'] = reviewed_scenario_df['answer_annotation']\n",
    "\n",
    "nlu_data_df[(nlu_data_df['scenario'].str.contains('alarm')) & (nlu_data_df['answer_annotation'].str.contains(\n",
    "    'event_name'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark changed data set\n",
    "TODO: repeat reports for the changed data set for domain and entities and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_reviewed_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=nlu_data_df)\n",
    "entity_reviewed_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure it is okay, you can save it as a csv file, make sure to name it correctly(i.e. `alarm_domain_first_review.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_scenario_df.to_csv('alarm_domain_first_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back up and check to make sure it looks okay. Make sure to give it the right name!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_domain_first_review_df = pd.read_csv(\n",
    "    'alarm_domain_first_review.csv', index_col=0)\n",
    "audio_domain_first_review_df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the evaluate_classifier in the NLU engine to check f1 score for intents and entities in the domain vs original NLU data of domain!\n",
    "# Value: benchmark!\n",
    "#TODO: implement a flow for getting the domains with the lowest f1 scores by intent/domain and entities and cleaning them by the order of the lowest f1 scores\n",
    "# TODO: concat all reviewed dfs and save to csv\n",
    "# TODO: add benchmark for whole NLU data set before and after cleaning! (by intents and domains!)\n",
    "# TODO: review the review marked entries\n",
    "# TODO: add new column for notes\n",
    "# TODO: change flow of review for only ones that should be reviewed, not all of the ones that have been changed (track changes by comparing against the original data set)\n",
    "# TODO: do the changed utterances have to be changed in other fields too or is it just enough for the tagged utterancve field?\n",
    "# TODO: add visualizations of domains, their intents, keywords in utterances, and entities to top\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
