{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu_engine import Analytics\n",
    "from nlu_engine import RenderJSON\n",
    "from nlu_engine import DataUtils\n",
    "from nlu_engine import MacroEntityRefinement\n",
    "from nlu_engine import MacroDataRefinement\n",
    "from nlu_engine import NLUEngine\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from nlu_engine import EntityExtractor, crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug report\n",
    "\n",
    "When running `cross_val_predict` from sklearn on the entity extractor for `general` domain, the following error is encountered:\n",
    "\n",
    "```\n",
    "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click here for more info. View Jupyter log for further details.\n",
    "\n",
    "Canceled future for execute_request message before replies were done\n",
    "```\n",
    "\n",
    "It's funny because if you run `cross_val_predict` on the entity extractor for any other domain, it works perfectly. Even funnier, if you run it on all of the domains together, it ALSO works! \n",
    "\n",
    "That doesn't seem logical to me. Why would it work on every domain separately except for `general` but it works when you run them together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our data that causes the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = pd.read_csv(\n",
    "        'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)\n",
    "\n",
    "nlu_data_df = DataUtils.convert_annotated_utterances_to_normalised_utterances(\n",
    "    nlu_data_df)\n",
    "\n",
    "removed_nlu_data_refined_df = nlu_data_df[nlu_data_df['remove'] != True]\n",
    "\n",
    "domain = 'general'\n",
    "domain_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == domain]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run `cross_val_predict` on the entity extractor for all domains together, it works perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating entity classifier\n",
      "Cross validating with CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
      "    keep_tempfiles=None, max_iterations=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to cross validate CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
      "    keep_tempfiles=None, max_iterations=100): 272.7438073158264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=removed_nlu_data_refined_df, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = EntityExtractor.get_targets_and_labels(domain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where it goes wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Analytics.cross_validate_classifier(crf, X, y, cv=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
