{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu_engine import EntityExtractor, crf\n",
    "from nlu_engine import Analytics\n",
    "from nlu_engine import RenderJSON\n",
    "from nlu_engine import DataUtils\n",
    "from nlu_engine import MacroEntityRefinement\n",
    "from nlu_engine import MacroDataRefinement\n",
    "\n",
    "from nlu_engine import NLUEngine\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: redo intro to be specific for general reports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Entity Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, you’ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "NOTE: We will use Logistic Regression with TFIDF features to train our intent models and CRFs for entity exraction. Why? Well, they are very fast and both methods aren't state-of-the-art. This is good, because it is easier to find problems we will need to refine in the dataset than if we were to use a proper NLU engine like Snips or something SOTA like BERT. It is very important to note that some of the the problems we will pick up on, might not be an actual issue, but might be due to the limitations of the models. Refining the real problems and ignoring the limitations of the models is a good way to improve the models. Then when the dataset is ready, we can use some more advanced NLU engine and get the best performance possible.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: figure out what needs to be done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded nlu_data_refined_df.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlu_data_df = pd.read_csv(\n",
    "        'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)\n",
    "    print('Successfully loaded nlu_data_refined_df.csv')\n",
    "except:\n",
    "    data = 'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    "    nlu_data_df = DataUtils.load_data(\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `nlu_data_df['answer_normalised']` is taken from `nlu_data_df['answer_annotation']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = DataUtils.convert_annotated_utterances_to_normalised_utterances(\n",
    "    nlu_data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove the unwanted entries for the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_df[nlu_data_df['remove'] != True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity extraction report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a new entity report or load a previous one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a previous report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = pd.read_csv('data/reports/entity_report.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new entity report\n",
    "\n",
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=removed_nlu_data_refined_df, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the current report if we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df.to_csv(\n",
    "    'data/reports/entity_report.csv',\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to extract out the entity types from the dataset. We need this to calculate the precentage of entities, to filter out any domains that have too few entities for benchmarking by domain instead of all domains together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df['entities'] = nlu_data_df['answer_annotation'].apply(\n",
    "    EntityExtractor.extract_entities)\n",
    "\n",
    "nlu_data_df['entity_types'] = nlu_data_df['entities'].apply(\n",
    "    EntityExtractor.extract_entity_types)\n",
    "\n",
    "import numpy as np\n",
    "nlu_data_df['entity_types'] = nlu_data_df['entity_types'].apply(lambda y: np.nan if len(y)==0 else y)\n",
    "nlu_data_df['entities'] = nlu_data_df['entities'].apply(lambda y: np.nan if len(y)==0 else y)\n",
    "\n",
    "removed_nlu_data_df = nlu_data_df[nlu_data_df['remove'] != True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a look at the domain entity reports. We can load a previous report or create a new one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a previous report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = pd.read_csv('data/reports/domain_entity_reports.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = NLUEngine.get_entity_reports_for_domains(removed_nlu_data_refined_df)\n",
    "\n",
    "domain_entity_reports_df = domain_entity_reports_df[domain_entity_reports_df['entity-type'].str.contains(\n",
    "    'weighted avg')].sort_values(by='f1-score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity-type</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.644421</td>\n",
       "      <td>0.685911</td>\n",
       "      <td>0.659549</td>\n",
       "      <td>11051.0</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.684923</td>\n",
       "      <td>0.782016</td>\n",
       "      <td>0.725743</td>\n",
       "      <td>367.0</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.838414</td>\n",
       "      <td>0.846966</td>\n",
       "      <td>0.839535</td>\n",
       "      <td>2274.0</td>\n",
       "      <td>takeaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.844779</td>\n",
       "      <td>0.866175</td>\n",
       "      <td>0.852882</td>\n",
       "      <td>3527.0</td>\n",
       "      <td>recommendation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.884246</td>\n",
       "      <td>0.888836</td>\n",
       "      <td>0.885063</td>\n",
       "      <td>18819.0</td>\n",
       "      <td>calendar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.890012</td>\n",
       "      <td>0.892921</td>\n",
       "      <td>0.888886</td>\n",
       "      <td>4492.0</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.881286</td>\n",
       "      <td>0.900264</td>\n",
       "      <td>0.889026</td>\n",
       "      <td>2647.0</td>\n",
       "      <td>lists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.878675</td>\n",
       "      <td>0.905823</td>\n",
       "      <td>0.890492</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.909339</td>\n",
       "      <td>0.915642</td>\n",
       "      <td>0.912069</td>\n",
       "      <td>6508.0</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.915545</td>\n",
       "      <td>0.921031</td>\n",
       "      <td>0.916768</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>iot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.919165</td>\n",
       "      <td>0.922014</td>\n",
       "      <td>0.918569</td>\n",
       "      <td>4052.0</td>\n",
       "      <td>datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.918154</td>\n",
       "      <td>0.925614</td>\n",
       "      <td>0.919751</td>\n",
       "      <td>6762.0</td>\n",
       "      <td>transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.920187</td>\n",
       "      <td>345.0</td>\n",
       "      <td>audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.918911</td>\n",
       "      <td>0.923835</td>\n",
       "      <td>0.920545</td>\n",
       "      <td>10661.0</td>\n",
       "      <td>qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.929788</td>\n",
       "      <td>0.935575</td>\n",
       "      <td>0.931526</td>\n",
       "      <td>9251.0</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.949521</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.944100</td>\n",
       "      <td>2530.0</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     entity-type  precision    recall  f1-score  support          domain\n",
       "0   weighted avg   0.644421  0.685911  0.659549  11051.0            play\n",
       "1   weighted avg   0.684923  0.782016  0.725743    367.0           music\n",
       "2   weighted avg   0.838414  0.846966  0.839535   2274.0        takeaway\n",
       "3   weighted avg   0.844779  0.866175  0.852882   3527.0  recommendation\n",
       "4   weighted avg   0.884246  0.888836  0.885063  18819.0        calendar\n",
       "5   weighted avg   0.890012  0.892921  0.888886   4492.0            news\n",
       "6   weighted avg   0.881286  0.900264  0.889026   2647.0           lists\n",
       "7   weighted avg   0.878675  0.905823  0.890492   1975.0         cooking\n",
       "8   weighted avg   0.909339  0.915642  0.912069   6508.0         weather\n",
       "9   weighted avg   0.915545  0.921031  0.916768   4850.0             iot\n",
       "10  weighted avg   0.919165  0.922014  0.918569   4052.0        datetime\n",
       "11  weighted avg   0.918154  0.925614  0.919751   6762.0       transport\n",
       "12  weighted avg   0.919051  0.921739  0.920187    345.0           audio\n",
       "13  weighted avg   0.918911  0.923835  0.920545  10661.0              qa\n",
       "14  weighted avg   0.929788  0.935575  0.931526   9251.0           email\n",
       "15  weighted avg   0.949521  0.950988  0.944100   2530.0           alarm"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_entity_reports_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df.to_csv('data/reports/domain_entity_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create new notebook for domain specific entity reports and entity refinement\n",
    "#TODO: Domain specific entity refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>answerid</th>\n",
       "      <th>notes</th>\n",
       "      <th>question</th>\n",
       "      <th>suggested_entities</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>intent_refined</th>\n",
       "      <th>entity_refined</th>\n",
       "      <th>remove</th>\n",
       "      <th>status</th>\n",
       "      <th>answer_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>wake me up at 5am this week</td>\n",
       "      <td>wake me up at five am this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : five am] [date : this week]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>wake me up at 9am on Friday</td>\n",
       "      <td>wake me up at nine am on friday</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : nine am] on [date : friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set an alarm for [time : two hours from now]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>player_setting</td>\n",
       "      <td>Olly quiet!</td>\n",
       "      <td>quiet</td>\n",
       "      <td>audio</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quiet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>player_setting</td>\n",
       "      <td>Olly pause for ten seconds</td>\n",
       "      <td>pause for ten seconds</td>\n",
       "      <td>audio</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pause for [time : ten seconds]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>781.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25712</th>\n",
       "      <td>NaN</td>\n",
       "      <td>782.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25713</th>\n",
       "      <td>NaN</td>\n",
       "      <td>783.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25714</th>\n",
       "      <td>NaN</td>\n",
       "      <td>784.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25715</th>\n",
       "      <td>NaN</td>\n",
       "      <td>785.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25074 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  answerid notes  \\\n",
       "0         1.0       1.0   NaN   \n",
       "1         1.0       2.0   NaN   \n",
       "2         1.0       3.0   NaN   \n",
       "3         1.0      31.0   NaN   \n",
       "5         1.0      33.0   NaN   \n",
       "...       ...       ...   ...   \n",
       "25711     NaN     781.0   NaN   \n",
       "25712     NaN     782.0   NaN   \n",
       "25713     NaN     783.0   NaN   \n",
       "25714     NaN     784.0   NaN   \n",
       "25715     NaN     785.0   NaN   \n",
       "\n",
       "                                                question suggested_entities  \\\n",
       "0      Write what you would tell your PDA in the foll...         date, time   \n",
       "1      Write what you would tell your PDA in the foll...         date, time   \n",
       "2      Write what you would tell your PDA in the foll...         date, time   \n",
       "3      Write what you would tell your PDA in the foll...     player_setting   \n",
       "5      Write what you would tell your PDA in the foll...     player_setting   \n",
       "...                                                  ...                ...   \n",
       "25711                                                NaN                NaN   \n",
       "25712                                                NaN                NaN   \n",
       "25713                                                NaN                NaN   \n",
       "25714                                                NaN                NaN   \n",
       "25715                                                NaN                NaN   \n",
       "\n",
       "                                      answer  \\\n",
       "0                wake me up at 5am this week   \n",
       "1                wake me up at 9am on Friday   \n",
       "2        set an alarm for two hours from now   \n",
       "3                                Olly quiet!   \n",
       "5                 Olly pause for ten seconds   \n",
       "...                                      ...   \n",
       "25711  that's cool, musch appreciated, olly.   \n",
       "25712             you are hero, appreciated.   \n",
       "25713                   thanks, that's nice.   \n",
       "25714        that's cool, thank you so much.   \n",
       "25715         appreciate your answers, olly.   \n",
       "\n",
       "                           answer_normalised scenario       intent  \\\n",
       "0            wake me up at five am this week    alarm    alarm_set   \n",
       "1            wake me up at nine am on friday    alarm    alarm_set   \n",
       "2        set an alarm for two hours from now    alarm    alarm_set   \n",
       "3                                      quiet    audio  volume_mute   \n",
       "5                      pause for ten seconds    audio  volume_mute   \n",
       "...                                      ...      ...          ...   \n",
       "25711  that's cool, musch appreciated, olly.  general       praise   \n",
       "25712             you are hero, appreciated.  general       praise   \n",
       "25713                   thanks, that's nice.  general       praise   \n",
       "25714        that's cool, thank you so much.  general       praise   \n",
       "25715         appreciate your answers, olly.  general       praise   \n",
       "\n",
       "      predicted_label intent_refined  entity_refined remove status  \\\n",
       "0           alarm_set            NaN             NaN    NaN    NaN   \n",
       "1           alarm_set            NaN             NaN    NaN    NaN   \n",
       "2           alarm_set            NaN             NaN    NaN    NaN   \n",
       "3         volume_mute            NaN             NaN    NaN    NaN   \n",
       "5         volume_mute           True             NaN   True    NaN   \n",
       "...               ...            ...             ...    ...    ...   \n",
       "25711          praise            NaN             NaN    NaN    NaN   \n",
       "25712          praise            NaN             NaN    NaN    NaN   \n",
       "25713          praise            NaN             NaN    NaN    NaN   \n",
       "25714          praise            NaN             NaN    NaN    NaN   \n",
       "25715          praise            NaN             NaN    NaN    NaN   \n",
       "\n",
       "                                       answer_annotation  \n",
       "0      wake me up at [time : five am] [date : this week]  \n",
       "1      wake me up at [time : nine am] on [date : friday]  \n",
       "2           set an alarm for [time : two hours from now]  \n",
       "3                                                  quiet  \n",
       "5                         pause for [time : ten seconds]  \n",
       "...                                                  ...  \n",
       "25711              that's cool, musch appreciated, olly.  \n",
       "25712                         you are hero, appreciated.  \n",
       "25713                               thanks, that's nice.  \n",
       "25714                    that's cool, thank you so much.  \n",
       "25715                     appreciate your answers, olly.  \n",
       "\n",
       "[25074 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_df[nlu_data_df['remove'] != True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: load domain input and get analytics of that domain\n",
    "#TODO: refine by that domain for each intent (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific entity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new domain start here!\n",
    "\n",
    "Pick a domain to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(\n",
    "    removed_nlu_data_refined_df)\n",
    "\n",
    "domain_df = DataUtils.get_domain_df(\n",
    "    removed_nlu_data_refined_df, domain_selection)\n",
    "\n",
    "domain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: domain specific entity analysis and refinement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
